Моделиране на данни представлява тестване на хипотези.

При ML
Имаме данни - хипотезата е, че могат да бъдат моделирани по този и този начин.
Какво искам да постигна или дали е правилно?
Pause and Ponder

Основната част от курса е тук:
Linear And Logistic Regression - как работи моделирането.
Model Training And Improvement - как се избират модели.

Дървета и ensemble models.
Support vector machines.

Тези 4 части са - Supervised Learning

Unsupervised learning:
Clustering
Dimensionality Reduction

Introduction to Neural Networks - предговор към следващия курс.
Най-големия проблем на Невронните мрежи, когато имаме таблични данни.
Time Series - времеви редове.

Задачи правени по време на курса - ще предадем заедно с курса.
За проекта - 70% - проекта
             30% - знания по теория.
			 
20% - paper summaries - обобщения на 2 научни статии, свързани с машинно самообучение.
Да можем да прочетем, разберем и репродуцираме части от научни статии.

Какво е научна статия ще говорим по време на курса.

Материали - 
Youtube канали - вършат страхотна работа (линкове в 00.Course Introduction).
math_guide.txt

Hardcore математика: The Elements of Statistical Learning - безплатна книга. 
//Hands-on Machine Learning with Scikit-Learn, Keras & Tensor Flow - само ако сме ужасно начинаещи.
За тези с инженерна насоченост - Designing Machine Learning Systems - An Iterative Process for Production-Ready Applications - 
има неща свързани с Operations. 
Andriy Burkov - The Hundred-Page Machine Learning Book. 

Q&A
======================
Резюметата може да корелират с проекта
arxiv.org - научна статия означава документ, който е минал peer review.
Може да бъде статия за Machine Learning, Статия която ползва Machine Learning за друга цел. Искали са да има една статия насочена към теория, и една към практика.
Правим резюме на статията, за да отбележим основните моменти в нея. После репродуцираме максимална част от резултатите в нея - тоест намираме данните, тренираме модели със същите данни които има в статията и трябва да получим същите резултати.

Peer review - не е задължително да е научен комитет.
openreview.net - 
Популярни статии - "Deep Image Prior", "An Image is Worth 16x16 Words"
paperswithcode.com

Weather Image Restoration.
https://arxiv.org/abs/2408.08459v1
loss функция

как се оценяват методи.
Може да се наложи да открием нови Use Cases.

Съдържанието на курса е това което говорим. Презентацията е минимума.
obsidian за водене на бележки
OneNote.


=================================================================================================================================== 1:10:00

Научен метод - начин за създаване на хипотези и взимане на решения - The scientific method steps. - 1-Introduction-to-Machine-Learning.pdf - p.5
Започваме с въпрос. Каквито и данни да имаме, ако нямаме нещо което да ни интересува за тях - нямаме нищо.
Този въпрос - първа задача е да го трансформираме в научен въпрос. Ако задаваме въпрос, който може да бъде проверен, и даваме отговор, който може да бъде отхвърлен.
Например - искаме да получим повече пари от бизнеса си. Кой продукт да пусна на промоция, че да направя макс печалба.

!!! Предварително проучаване по темата - да разберем какво са правили други хора преди нас, евентуално със същите данни....какви методи съществуват.
Като имаме въпрос харчим известно време да разберем по какви начин е решаван този въпрос или подобни на него.
В статията - Related work. Сравняване какво е правено до момента.
 
На база на въпроса, който съм задал, и на това което вече знам като информация, формулирам конкретен въпрос, чиито отговор е да или не. И отговарям на въпроса с предполагаемо да. Но тази хипотеза трябва да може да бъде отхвърлена. Всеки един machine learning модел е хипотеза за данни, Всеки един machine learning модел представлява хипотези, че данните ни могат да бъдат моделирани при тези и тези условия с такава и такава метрика за точност, която може и да бъде комплексна.

След това тестваме
Анализираме данни, пускаме си модели. Трансформираме ги. променямаме dataset, feature engineering, extraction. 
И в някакъв момент бидейки готови с някакъв експеримент, анализираме резултатите от него. - имаме експеримент, пускаме го на много на брой пъти и сравняваме резултатие.


Ок модела ми не е добър, дай да го променя.
Нова хипотеза, нов модел - евентуално нова обработка за данни. 
Много сме ларж откъм outlier-и.
Целият процес е много силно итеративен.
В един момент резултатите ни задоволяват и показваме резултати.
!!! Обаче - reporting - има и в двата случая.
Ще трябва да си рефинираме хипотезите и да си рафинираме хипотезите.
Данните идват чак при - Test with Experiment.
Процесът е много итеративен
OSEMN


Това се нарича KDD - Knowledge Discovery in Data
Винаги минаваме през това, особено когато не е надграждане на съществуващ процес, ами е нов research.
Или нов метод върху стари данни..всичко което не е просто резултат от код.

Ако сте правили тестове/дебъзи на нещо, сте минавали през този процес.

Трябва да имаме обучение, трябва да имаме задача, трябва да имаме метрика която проследява колко добре се справяме върху тази задача. Ако имаме грешна метрика, измерваме грешно нешо. И чакаме с натрупването на опит нашето парче код да показва все по-добра и по-добра метрика, докато стане достатъчно добре.
Имаме процес на обучение.

Разяснение на дефинициите:
experience - данни
performance measure - метрика или метрики за модела, за да видим колко добре се е научил.
task - loss function - задача.

когато метрика става достатъчно добра, смятаме че алгоритъма се е обучил.
За разлика от алгоритъма.

Нашите алгоритми не правят нищо преди да получат данни.

Имаме предположение, имаме начин за работа и ето ни линейна регресия - това е алгоритъма, но когато го приложим върху данни, получаваме модел за нашите данни.
Има разлика между machine learning модел и machine learning algorithm. 

Има 3 типа Machine Learning Algorithms:

- Supervised learning
- Unsupervised learning
- Reinforcement learning

Supervised learning
=================================
Имаме входни параметри и очакваме резултат. Сравняваме това което казва нашия модел с това което очакваме. Искаме нашия machine learning модел да наподобява възможно най-много данните, които сме видели.


Unsupervised learning
=================================
Имаме входни данни, нямаме очакван резултат. Това означава че няма с какво да сравним резултата от модела, обаче имаме някакво предположение за данните, имаме някаква хипотеза.

Целта не е да разберем процеса, ами да го имитираме.
Нашата работа е да направим функция, на която като подадем същите данни, резултата ще го наречем y tilda
features са самите колони на dataset.



Y nx1 - target, observation
y tilda - model output, prediction

Може да предсказваме, моделираме повече от 1 променлива.
Искаме y tilda да е подобно на y. 
Може да добавим допълните условия.
Въпросът е какво означава подобно.Подобието е мярка за разстояние. 
Разстояние:
1. > 0
2. = 0 - само тогава, когато двете неща са равни.
3. Разстоянието спазва правилото на триъгълника. - AB не може да е по-дълга от AC + BC.- неравенство на коши.

Има много начини за дефиниране на подобие между 2 елемента и точно тези начини дават нашата loss функция.
За момента ще ползваме задачи, които са готови, ама после сами ще трябва да си мислим loss функциите.

!!! Начините по които дефинираме това подобие ще ни каже каква задача решаваме. Това е начинът един и същи модел да решава различни задачи. (в зелено).
Това което пишем, е че дефинираме разстояние d(yi, ytildai) и го усредняваме. - функция на грешката.
Какво става при Unsupervised learning - там нямаме y. - доколко y tilda отговаря на хипотезата, която имаме. 
hypothesis testing. 

При Reinforcement learning - обучение с повторение - модел, който като се обучат, променят средата си.
Моделите се наричат агенти - те не моделират данни, ами предприемат действа. 
Пример - самоуправляваща се кола. 
Алгоритъма променя собствената си среда.
Може да имаме много агенти, които действат накуп.
Reinforcement се постига най-често с комбинация от Supervised learning и Unsupervised learning.
В съвременният свят имаме често комбинации на двете.
Проблеми : при моделирането на данни.
- проблемни данни - лошо обучение - модел който пропуска нещо
- малко време за обучение - оптимизация която не е завършила.
- грешна loss функция
- грешно подаване на данни.


Има още един клас алгоритми - metalearning - обучение за самото обучение. 
Cross validation е такъв алгоритъма.
Това е алгоритъм който позволява между различните модели кой се справя по-добре.
Алгоритми за grade and descend.
Няма нужда да подаваме всички данни наведнъж. 

Regression - задача, за която y e реално число. Тука имаме числова променлива. 
Classification - задача, в която y e някакво множество с краен брой елементи. Тука има категорийна променлива. 

Резултатите, които получаваме, много зависят от това колко разбираме данните, и условията, при които са били получени и при които работят.
Непознаването на данните водят до лоши резултати.

Принципът е Garbage in - garbage out.
Функциите ни са на няколко милиарда променливи.
Как търсим локални екстремуми - 
Нека имаме y = f(x), където x = 30k записа на 5000 елемента.
Трябва да вземем производна на функция. Само дето не съществува производна на 5000 променливи по дефиниция.
Така че имаме градиент - който е вектор с 5000 елемента - производна на функцията спрямо първата променлива....до 5-хилядната променлива.
Ще ни трябва много време да ни отнеме да видим кога този градиент е = 0 (максимума)..може би повече от цялото време на вселената.
Затова ползваме алгоритъм който апроксимира.
Прибягваме до алчен алгоритъм. Той се опитва да оптимизира. 
Критично важно е че не знам всички възможни решения. 
Не е гарантирано че е правилен. 


Работа с данни
===========================================
Освен да си документираме процеса, има още 2 неща, които се правят в machine learning:
1. Да нормализираме стойностите и да ги скалираме по някакъв начин.
2. Ако имаме категории много често ги превръщаме в индикаторни променливи. 

Може да се наложи да си ограничим избора на модели.
Ако имаме класификация с небалансирани данни - така да направим, че модела да разпознава и двата класа. - boolstrapping.
Почти всички функции имат default параметри, което означава ще връща някакви резултати.

Какво е gradient descent - начин за оптимизация на функции, използваме го за търсене на локални минимуми близо до начална точка.
Искаме разстоянието между модели и наблюдения да бъде минимална. 

kaggle - имаше dataset за x ray.

Кое наричаме feature и кое наричаме target зависи от нас.




















			

