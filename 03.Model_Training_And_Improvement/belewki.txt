Изборът на модел е точна наука - трябва да минимизираме, оптимизираме някаква loss функцията - какво искаме да добавим към него, добавяме към loss функцията.
Обаче начинът да разберем какво работи на практика е експериментален - да успеем да избегнем bias, или проблеми, с които може да се сблъскаме са експериментални

Learnability problem - може ли някаква функция, някакъв алгоритъм за ML да фитне за какъвто и да е Data Set. Отговорът е че не може - доказано нерешим.
Никоя система не може да опише сама себе си - никой модел не може да каже дали върши работата си правилно или не...

Когато избираме настройки трябва да знаем какво ни е на нас по-важно от друго.
Има и случаи, при които настройки на хипер параметри може да го намалим. - например няма време да го правим и нямаме време.

Трябва да знаем какво влиза в някакви решения, за да знаем как да ги направим.
Reproducible research, evidence based research.

В нашият случай evidence са данните или резултат от това което са правили хора преди нас.
evidence based research означава да правим избори, които са обосновани, и които първо - може да кажем защо са направени, и второ може да поставим под въпрос дали предположенията за тези избори са правилни.

сплит 70:30 е подходящ при малки данни. при 1 милион записа това означава 300 000 за тестване, което е твърде много. Други са предположенията и причините, поради които искаме процент или брой dataset за тестване.

Когато имаме случайни данни, имаме тенденцията да виждаме форми в тях. Например pareidoliа.
Търсим закономерности и много лесно ги виждаме дори и да ги няма. Може да е зле, когато ги търсим нарочно, и в най-лошия случай когато се опитваме да нагласим данните спрямо предположенията си - Cherry Picking. 
Повече на брой измерения почват да създават проблеми. Говорим и за шум в данните - или всичко е шум, или шума спрямо полезния сигнал е много силен. Това е особено валидно при много на брой измерения.


